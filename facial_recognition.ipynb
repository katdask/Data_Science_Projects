{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "disciplinary-large",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import optimizers\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "rough-speaker",
   "metadata": {},
   "outputs": [],
   "source": [
    "celebs=pd.read_csv(\"facial_recognition/list_attr_celeba.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "psychological-volume",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>5_o_Clock_Shadow</th>\n",
       "      <th>Arched_Eyebrows</th>\n",
       "      <th>Attractive</th>\n",
       "      <th>Bags_Under_Eyes</th>\n",
       "      <th>Bald</th>\n",
       "      <th>Bangs</th>\n",
       "      <th>Big_Lips</th>\n",
       "      <th>Big_Nose</th>\n",
       "      <th>Black_Hair</th>\n",
       "      <th>...</th>\n",
       "      <th>Sideburns</th>\n",
       "      <th>Smiling</th>\n",
       "      <th>Straight_Hair</th>\n",
       "      <th>Wavy_Hair</th>\n",
       "      <th>Wearing_Earrings</th>\n",
       "      <th>Wearing_Hat</th>\n",
       "      <th>Wearing_Lipstick</th>\n",
       "      <th>Wearing_Necklace</th>\n",
       "      <th>Wearing_Necktie</th>\n",
       "      <th>Young</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000002.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000003.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000004.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000005.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     image_id  5_o_Clock_Shadow  Arched_Eyebrows  Attractive  Bags_Under_Eyes  \\\n",
       "0  000001.jpg                -1                1           1               -1   \n",
       "1  000002.jpg                -1               -1          -1                1   \n",
       "2  000003.jpg                -1               -1          -1               -1   \n",
       "3  000004.jpg                -1               -1           1               -1   \n",
       "4  000005.jpg                -1                1           1               -1   \n",
       "\n",
       "   Bald  Bangs  Big_Lips  Big_Nose  Black_Hair  ...  Sideburns  Smiling  \\\n",
       "0    -1     -1        -1        -1          -1  ...         -1        1   \n",
       "1    -1     -1        -1         1          -1  ...         -1        1   \n",
       "2    -1     -1         1        -1          -1  ...         -1       -1   \n",
       "3    -1     -1        -1        -1          -1  ...         -1       -1   \n",
       "4    -1     -1         1        -1          -1  ...         -1       -1   \n",
       "\n",
       "   Straight_Hair  Wavy_Hair  Wearing_Earrings  Wearing_Hat  Wearing_Lipstick  \\\n",
       "0              1         -1                 1           -1                 1   \n",
       "1             -1         -1                -1           -1                -1   \n",
       "2             -1          1                -1           -1                -1   \n",
       "3              1         -1                 1           -1                 1   \n",
       "4             -1         -1                -1           -1                 1   \n",
       "\n",
       "   Wearing_Necklace  Wearing_Necktie  Young  \n",
       "0                -1               -1      1  \n",
       "1                -1               -1      1  \n",
       "2                -1               -1      1  \n",
       "3                 1               -1      1  \n",
       "4                -1               -1      1  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "celebs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "rising-nudist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>5_o_Clock_Shadow</th>\n",
       "      <th>Arched_Eyebrows</th>\n",
       "      <th>Attractive</th>\n",
       "      <th>Bags_Under_Eyes</th>\n",
       "      <th>Bald</th>\n",
       "      <th>Bangs</th>\n",
       "      <th>Big_Lips</th>\n",
       "      <th>Big_Nose</th>\n",
       "      <th>Black_Hair</th>\n",
       "      <th>...</th>\n",
       "      <th>Sideburns</th>\n",
       "      <th>Smiling</th>\n",
       "      <th>Straight_Hair</th>\n",
       "      <th>Wavy_Hair</th>\n",
       "      <th>Wearing_Earrings</th>\n",
       "      <th>Wearing_Hat</th>\n",
       "      <th>Wearing_Lipstick</th>\n",
       "      <th>Wearing_Necklace</th>\n",
       "      <th>Wearing_Necktie</th>\n",
       "      <th>Young</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>facial_recognition/img_align_celeb/000001.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>facial_recognition/img_align_celeb/000002.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>facial_recognition/img_align_celeb/000003.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>facial_recognition/img_align_celeb/000004.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>facial_recognition/img_align_celeb/000005.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        image_id  5_o_Clock_Shadow  \\\n",
       "0  facial_recognition/img_align_celeb/000001.jpg                -1   \n",
       "1  facial_recognition/img_align_celeb/000002.jpg                -1   \n",
       "2  facial_recognition/img_align_celeb/000003.jpg                -1   \n",
       "3  facial_recognition/img_align_celeb/000004.jpg                -1   \n",
       "4  facial_recognition/img_align_celeb/000005.jpg                -1   \n",
       "\n",
       "   Arched_Eyebrows  Attractive  Bags_Under_Eyes  Bald  Bangs  Big_Lips  \\\n",
       "0                1           1               -1    -1     -1        -1   \n",
       "1               -1          -1                1    -1     -1        -1   \n",
       "2               -1          -1               -1    -1     -1         1   \n",
       "3               -1           1               -1    -1     -1        -1   \n",
       "4                1           1               -1    -1     -1         1   \n",
       "\n",
       "   Big_Nose  Black_Hair  ...  Sideburns  Smiling  Straight_Hair  Wavy_Hair  \\\n",
       "0        -1          -1  ...         -1        1              1         -1   \n",
       "1         1          -1  ...         -1        1             -1         -1   \n",
       "2        -1          -1  ...         -1       -1             -1          1   \n",
       "3        -1          -1  ...         -1       -1              1         -1   \n",
       "4        -1          -1  ...         -1       -1             -1         -1   \n",
       "\n",
       "   Wearing_Earrings  Wearing_Hat  Wearing_Lipstick  Wearing_Necklace  \\\n",
       "0                 1           -1                 1                -1   \n",
       "1                -1           -1                -1                -1   \n",
       "2                -1           -1                -1                -1   \n",
       "3                 1           -1                 1                 1   \n",
       "4                -1           -1                 1                -1   \n",
       "\n",
       "   Wearing_Necktie  Young  \n",
       "0               -1      1  \n",
       "1               -1      1  \n",
       "2               -1      1  \n",
       "3               -1      1  \n",
       "4               -1      1  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "celebs[\"image_id\"]='facial_recognition/img_align_celeb/' + celebs[\"image_id\"].astype(str)\n",
    "celebs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "vertical-librarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "celebs.replace(-1, 0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "circular-colonial",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>5_o_Clock_Shadow</th>\n",
       "      <th>Arched_Eyebrows</th>\n",
       "      <th>Attractive</th>\n",
       "      <th>Bags_Under_Eyes</th>\n",
       "      <th>Bald</th>\n",
       "      <th>Bangs</th>\n",
       "      <th>Big_Lips</th>\n",
       "      <th>Big_Nose</th>\n",
       "      <th>Black_Hair</th>\n",
       "      <th>...</th>\n",
       "      <th>Sideburns</th>\n",
       "      <th>Smiling</th>\n",
       "      <th>Straight_Hair</th>\n",
       "      <th>Wavy_Hair</th>\n",
       "      <th>Wearing_Earrings</th>\n",
       "      <th>Wearing_Hat</th>\n",
       "      <th>Wearing_Lipstick</th>\n",
       "      <th>Wearing_Necklace</th>\n",
       "      <th>Wearing_Necktie</th>\n",
       "      <th>Young</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>facial_recognition/img_align_celeb/004996.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>facial_recognition/img_align_celeb/004997.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>facial_recognition/img_align_celeb/004998.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>facial_recognition/img_align_celeb/004999.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>facial_recognition/img_align_celeb/005000.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           image_id  5_o_Clock_Shadow  \\\n",
       "4995  facial_recognition/img_align_celeb/004996.jpg                 0   \n",
       "4996  facial_recognition/img_align_celeb/004997.jpg                 0   \n",
       "4997  facial_recognition/img_align_celeb/004998.jpg                 0   \n",
       "4998  facial_recognition/img_align_celeb/004999.jpg                 0   \n",
       "4999  facial_recognition/img_align_celeb/005000.jpg                 0   \n",
       "\n",
       "      Arched_Eyebrows  Attractive  Bags_Under_Eyes  Bald  Bangs  Big_Lips  \\\n",
       "4995                0           0                1     0      0         1   \n",
       "4996                0           1                0     0      0         0   \n",
       "4997                0           1                0     0      1         1   \n",
       "4998                0           0                1     0      0         0   \n",
       "4999                1           1                0     0      0         1   \n",
       "\n",
       "      Big_Nose  Black_Hair  ...  Sideburns  Smiling  Straight_Hair  Wavy_Hair  \\\n",
       "4995         1           1  ...          0        0              1          0   \n",
       "4996         0           0  ...          0        0              0          0   \n",
       "4997         0           0  ...          0        1              0          1   \n",
       "4998         1           0  ...          0        1              0          0   \n",
       "4999         0           1  ...          0        1              0          0   \n",
       "\n",
       "      Wearing_Earrings  Wearing_Hat  Wearing_Lipstick  Wearing_Necklace  \\\n",
       "4995                 0            0                 0                 0   \n",
       "4996                 0            1                 0                 0   \n",
       "4997                 1            0                 1                 0   \n",
       "4998                 0            0                 0                 0   \n",
       "4999                 1            0                 1                 0   \n",
       "\n",
       "      Wearing_Necktie  Young  \n",
       "4995                0      0  \n",
       "4996                0      1  \n",
       "4997                0      1  \n",
       "4998                1      0  \n",
       "4999                0      1  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "celebs=celebs.iloc[0:5000,:]\n",
    "celebs.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "uniform-dress",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(218, 178, 3)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img=mpimg.imread(celebs[\"image_id\"][0])\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "conservative-trial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create X & normalization\n",
    "\n",
    "#X=np.empty([218, 178, 3])\n",
    "my_list=[]\n",
    "for i in range(5000):\n",
    "    img=mpimg.imread(celebs[\"image_id\"][i])\n",
    "    my_list.append(img/255)\n",
    "    \n",
    "#plt.imshow(img) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "endangered-haiti",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_array = np.array(my_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "figured-cameroon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.92941176, 0.94509804, 0.95686275],\n",
       "        [0.92941176, 0.94509804, 0.95686275],\n",
       "        [0.92941176, 0.94509804, 0.95686275],\n",
       "        ...,\n",
       "        [0.92941176, 0.93333333, 0.94901961],\n",
       "        [0.92941176, 0.93333333, 0.94901961],\n",
       "        [0.92941176, 0.93333333, 0.94901961]],\n",
       "\n",
       "       [[0.92941176, 0.94509804, 0.95686275],\n",
       "        [0.92941176, 0.94509804, 0.95686275],\n",
       "        [0.92941176, 0.94509804, 0.95686275],\n",
       "        ...,\n",
       "        [0.92941176, 0.93333333, 0.94901961],\n",
       "        [0.92941176, 0.93333333, 0.94901961],\n",
       "        [0.92941176, 0.93333333, 0.94901961]],\n",
       "\n",
       "       [[0.9254902 , 0.94509804, 0.95686275],\n",
       "        [0.9254902 , 0.94509804, 0.95686275],\n",
       "        [0.9254902 , 0.94509804, 0.95686275],\n",
       "        ...,\n",
       "        [0.92941176, 0.93333333, 0.94901961],\n",
       "        [0.92941176, 0.93333333, 0.94901961],\n",
       "        [0.92941176, 0.93333333, 0.94901961]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.9254902 , 0.8627451 , 0.77254902],\n",
       "        [0.96078431, 0.89411765, 0.81568627],\n",
       "        [0.9372549 , 0.87843137, 0.80392157],\n",
       "        ...,\n",
       "        [0.13333333, 0.12941176, 0.12156863],\n",
       "        [0.14117647, 0.1254902 , 0.12156863],\n",
       "        [0.14117647, 0.1254902 , 0.12156863]],\n",
       "\n",
       "       [[0.94509804, 0.88235294, 0.79215686],\n",
       "        [0.96470588, 0.89803922, 0.81960784],\n",
       "        [0.94509804, 0.88627451, 0.81176471],\n",
       "        ...,\n",
       "        [0.1254902 , 0.12156863, 0.11372549],\n",
       "        [0.1254902 , 0.10980392, 0.10588235],\n",
       "        [0.1254902 , 0.10980392, 0.10588235]],\n",
       "\n",
       "       [[0.94509804, 0.88235294, 0.79215686],\n",
       "        [0.96470588, 0.89803922, 0.81960784],\n",
       "        [0.94509804, 0.88627451, 0.81176471],\n",
       "        ...,\n",
       "        [0.1254902 , 0.12156863, 0.11372549],\n",
       "        [0.12941176, 0.10588235, 0.10588235],\n",
       "        [0.12941176, 0.10588235, 0.10588235]]])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_array[4999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "underlying-baltimore",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 218, 178, 3)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "following-reverse",
   "metadata": {},
   "source": [
    "### Classification for Attribute \"Male\" (binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "liable-bicycle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Male\n",
       "0    2917\n",
       "1    2083\n",
       "dtype: int64"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will first try an \"easier\" classification \"Male\"\n",
    "y_male=celebs[\"Male\"]\n",
    "celebs.groupby('Male').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "noticed-starter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train - test split\n",
    "\n",
    "X_train, X_test, y_male_train, y_male_test = train_test_split(my_array, y_male, test_size=0.20, random_state=82, stratify=y_male)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "light-sociology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 218, 178, 3)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "linear-essence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 218, 178, 3)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "inappropriate-graphic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000,)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_male_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "searching-times",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_male_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "pressed-sword",
   "metadata": {},
   "outputs": [],
   "source": [
    "# built the model\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Conv2D(32, 3, activation=\"relu\", input_shape=(218, 178, 3), name=\"Conv1\"))\n",
    "model.add(layers.MaxPooling2D(2,2))\n",
    "model.add(layers.Conv2D(64, 3, activation=\"relu\", name=\"Conv2\"))\n",
    "model.add(layers.MaxPooling2D(2,2))\n",
    "model.add(layers.Conv2D(128, 3, activation=\"relu\", name=\"Conv3\"))\n",
    "model.add(layers.MaxPooling2D(2,2))\n",
    "model.add(layers.Conv2D(256, 3, activation=\"relu\", name=\"Conv4\"))\n",
    "model.add(layers.MaxPooling2D(2,2))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation=\"relu\", name=\"Dense1\"))\n",
    "model.add(layers.Dense(64, activation=\"relu\", name=\"Dense2\"))\n",
    "model.add(layers.Dense(2, activation=\"softmax\", name=\"Prediction\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "superb-coalition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv1 (Conv2D)               (None, 216, 176, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 108, 88, 32)       0         \n",
      "_________________________________________________________________\n",
      "Conv2 (Conv2D)               (None, 106, 86, 64)       18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 53, 43, 64)        0         \n",
      "_________________________________________________________________\n",
      "Conv3 (Conv2D)               (None, 51, 41, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 25, 20, 128)       0         \n",
      "_________________________________________________________________\n",
      "Conv4 (Conv2D)               (None, 23, 18, 256)       295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 11, 9, 256)        0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 25344)             0         \n",
      "_________________________________________________________________\n",
      "Dense1 (Dense)               (None, 256)               6488320   \n",
      "_________________________________________________________________\n",
      "Dense2 (Dense)               (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "Prediction (Dense)           (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 6,893,314\n",
      "Trainable params: 6,893,314\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "sporting-russell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We specify the training configuration (optimizer, loss, metrics):\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(learning_rate=0.001),  \n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "senior-bearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    os.mkdir('checkpoint2')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "bigger-cigarette",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'best_model_male.h5'\n",
    "checkpoint_path= os.path.join('checkpoint2',file_name)\n",
    "\n",
    "call_back = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, \n",
    "                                                 monitor='val_sparse_categorical_accuracy', \n",
    "                                                 verbose=1,\n",
    "                                                 save_freq='epoch',\n",
    "                                                 save_best_only=True, \n",
    "                                                 save_weights_only=False, \n",
    "                                                 mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "ordinary-december",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model on training data\n",
      "Epoch 1/10\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6760 - sparse_categorical_accuracy: 0.6350\n",
      "Epoch 00001: val_sparse_categorical_accuracy improved from -inf to 0.73625, saving model to checkpoint2\\best_model_male.h5\n",
      "100/100 [==============================] - 113s 1s/step - loss: 0.6760 - sparse_categorical_accuracy: 0.6350 - val_loss: 0.5393 - val_sparse_categorical_accuracy: 0.7362\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5064 - sparse_categorical_accuracy: 0.7747\n",
      "Epoch 00002: val_sparse_categorical_accuracy improved from 0.73625 to 0.84000, saving model to checkpoint2\\best_model_male.h5\n",
      "100/100 [==============================] - 117s 1s/step - loss: 0.5064 - sparse_categorical_accuracy: 0.7747 - val_loss: 0.3582 - val_sparse_categorical_accuracy: 0.8400\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3249 - sparse_categorical_accuracy: 0.8634\n",
      "Epoch 00003: val_sparse_categorical_accuracy did not improve from 0.84000\n",
      "100/100 [==============================] - 115s 1s/step - loss: 0.3249 - sparse_categorical_accuracy: 0.8634 - val_loss: 0.4047 - val_sparse_categorical_accuracy: 0.7875\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2294 - sparse_categorical_accuracy: 0.9075\n",
      "Epoch 00004: val_sparse_categorical_accuracy improved from 0.84000 to 0.90875, saving model to checkpoint2\\best_model_male.h5\n",
      "100/100 [==============================] - 114s 1s/step - loss: 0.2294 - sparse_categorical_accuracy: 0.9075 - val_loss: 0.2291 - val_sparse_categorical_accuracy: 0.9087\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.1722 - sparse_categorical_accuracy: 0.9322\n",
      "Epoch 00005: val_sparse_categorical_accuracy improved from 0.90875 to 0.91250, saving model to checkpoint2\\best_model_male.h5\n",
      "100/100 [==============================] - 114s 1s/step - loss: 0.1722 - sparse_categorical_accuracy: 0.9322 - val_loss: 0.2597 - val_sparse_categorical_accuracy: 0.9125\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.1420 - sparse_categorical_accuracy: 0.9459\n",
      "Epoch 00006: val_sparse_categorical_accuracy improved from 0.91250 to 0.91500, saving model to checkpoint2\\best_model_male.h5\n",
      "100/100 [==============================] - 115s 1s/step - loss: 0.1420 - sparse_categorical_accuracy: 0.9459 - val_loss: 0.2105 - val_sparse_categorical_accuracy: 0.9150\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.1016 - sparse_categorical_accuracy: 0.9622\n",
      "Epoch 00007: val_sparse_categorical_accuracy improved from 0.91500 to 0.92250, saving model to checkpoint2\\best_model_male.h5\n",
      "100/100 [==============================] - 113s 1s/step - loss: 0.1016 - sparse_categorical_accuracy: 0.9622 - val_loss: 0.2882 - val_sparse_categorical_accuracy: 0.9225\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0826 - sparse_categorical_accuracy: 0.9691\n",
      "Epoch 00008: val_sparse_categorical_accuracy did not improve from 0.92250\n",
      "100/100 [==============================] - 112s 1s/step - loss: 0.0826 - sparse_categorical_accuracy: 0.9691 - val_loss: 0.7209 - val_sparse_categorical_accuracy: 0.8188\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0715 - sparse_categorical_accuracy: 0.9719\n",
      "Epoch 00009: val_sparse_categorical_accuracy improved from 0.92250 to 0.93500, saving model to checkpoint2\\best_model_male.h5\n",
      "100/100 [==============================] - 113s 1s/step - loss: 0.0715 - sparse_categorical_accuracy: 0.9719 - val_loss: 0.2463 - val_sparse_categorical_accuracy: 0.9350\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0656 - sparse_categorical_accuracy: 0.9753\n",
      "Epoch 00010: val_sparse_categorical_accuracy did not improve from 0.93500\n",
      "100/100 [==============================] - 114s 1s/step - loss: 0.0656 - sparse_categorical_accuracy: 0.9753 - val_loss: 0.3100 - val_sparse_categorical_accuracy: 0.9337\n"
     ]
    }
   ],
   "source": [
    "print(\"Fit model on training data\")\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_male_train,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    validation_split=0.2,\n",
    "    callbacks=call_back,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "discrete-texture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "32/32 [==============================] - 8s 238ms/step - loss: 0.2773 - sparse_categorical_accuracy: 0.9370\n",
      "test loss, test acc: [0.277332603931427, 0.9369999766349792]\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate(X_test, y_male_test, batch_size=32)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-wings",
   "metadata": {},
   "source": [
    "### Classification for attribute \"Blond Hair\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "lonely-stephen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Blond_Hair\n",
       "0    4270\n",
       "1     730\n",
       "dtype: int64"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_blond=celebs[\"Blond_Hair\"]\n",
    "celebs.groupby('Blond_Hair').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "overall-celtic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train - test split\n",
    "\n",
    "X_train, X_test, y_blond_train, y_blond_test = train_test_split(my_array, y_blond, test_size=0.20, random_state=82, stratify=y_blond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "latest-shield",
   "metadata": {},
   "outputs": [],
   "source": [
    "# built the model\n",
    "model_blond = keras.Sequential()\n",
    "model_blond.add(layers.Conv2D(32, 3, activation=\"relu\", input_shape=(218, 178, 3), name=\"Conv1\"))\n",
    "model_blond.add(layers.MaxPooling2D(2,2))\n",
    "model_blond.add(layers.Conv2D(64, 3, activation=\"relu\", name=\"Conv2\"))\n",
    "model_blond.add(layers.MaxPooling2D(2,2))\n",
    "model_blond.add(layers.Conv2D(128, 3, activation=\"relu\", name=\"Conv3\"))\n",
    "model_blond.add(layers.MaxPooling2D(2,2))\n",
    "model_blond.add(layers.Conv2D(256, 3, activation=\"relu\", name=\"Conv4\"))\n",
    "model_blond.add(layers.MaxPooling2D(2,2))\n",
    "model_blond.add(layers.Flatten())\n",
    "model_blond.add(layers.Dense(256, activation=\"relu\", name=\"Dense1\"))\n",
    "model_blond.add(layers.Dense(64, activation=\"relu\", name=\"Dense2\"))\n",
    "model_blond.add(layers.Dense(2, activation=\"softmax\", name=\"Prediction\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "racial-civilian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv1 (Conv2D)               (None, 216, 176, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 108, 88, 32)       0         \n",
      "_________________________________________________________________\n",
      "Conv2 (Conv2D)               (None, 106, 86, 64)       18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling (None, 53, 43, 64)        0         \n",
      "_________________________________________________________________\n",
      "Conv3 (Conv2D)               (None, 51, 41, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling (None, 25, 20, 128)       0         \n",
      "_________________________________________________________________\n",
      "Conv4 (Conv2D)               (None, 23, 18, 256)       295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling (None, 11, 9, 256)        0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 25344)             0         \n",
      "_________________________________________________________________\n",
      "Dense1 (Dense)               (None, 256)               6488320   \n",
      "_________________________________________________________________\n",
      "Dense2 (Dense)               (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "Prediction (Dense)           (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 6,893,314\n",
      "Trainable params: 6,893,314\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_blond.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "democratic-starter",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_blond.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(learning_rate=0.001),  \n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "rocky-toddler",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'best_model_blond.h5'\n",
    "checkpoint_path= os.path.join('checkpoint2',file_name)\n",
    "\n",
    "call_back = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, \n",
    "                                                 monitor='val_sparse_categorical_accuracy', \n",
    "                                                 verbose=1,\n",
    "                                                 save_freq='epoch',\n",
    "                                                 save_best_only=True, \n",
    "                                                 save_weights_only=False, \n",
    "                                                 mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "oriental-connectivity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model on training data\n",
      "Epoch 1/10\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4924 - sparse_categorical_accuracy: 0.8347\n",
      "Epoch 00001: val_sparse_categorical_accuracy improved from -inf to 0.89625, saving model to checkpoint2\\best_model_blond.h5\n",
      "100/100 [==============================] - 110s 1s/step - loss: 0.4924 - sparse_categorical_accuracy: 0.8347 - val_loss: 0.2755 - val_sparse_categorical_accuracy: 0.8963\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2817 - sparse_categorical_accuracy: 0.8853\n",
      "Epoch 00002: val_sparse_categorical_accuracy improved from 0.89625 to 0.91000, saving model to checkpoint2\\best_model_blond.h5\n",
      "100/100 [==============================] - 114s 1s/step - loss: 0.2817 - sparse_categorical_accuracy: 0.8853 - val_loss: 0.2171 - val_sparse_categorical_accuracy: 0.9100\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2312 - sparse_categorical_accuracy: 0.9094\n",
      "Epoch 00003: val_sparse_categorical_accuracy improved from 0.91000 to 0.91375, saving model to checkpoint2\\best_model_blond.h5\n",
      "100/100 [==============================] - 114s 1s/step - loss: 0.2312 - sparse_categorical_accuracy: 0.9094 - val_loss: 0.2478 - val_sparse_categorical_accuracy: 0.9137\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2018 - sparse_categorical_accuracy: 0.9216\n",
      "Epoch 00004: val_sparse_categorical_accuracy improved from 0.91375 to 0.92125, saving model to checkpoint2\\best_model_blond.h5\n",
      "100/100 [==============================] - 114s 1s/step - loss: 0.2018 - sparse_categorical_accuracy: 0.9216 - val_loss: 0.2237 - val_sparse_categorical_accuracy: 0.9212\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2011 - sparse_categorical_accuracy: 0.9331\n",
      "Epoch 00005: val_sparse_categorical_accuracy did not improve from 0.92125\n",
      "100/100 [==============================] - 116s 1s/step - loss: 0.2011 - sparse_categorical_accuracy: 0.9331 - val_loss: 0.2201 - val_sparse_categorical_accuracy: 0.9100\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.1676 - sparse_categorical_accuracy: 0.9409\n",
      "Epoch 00006: val_sparse_categorical_accuracy did not improve from 0.92125\n",
      "100/100 [==============================] - 114s 1s/step - loss: 0.1676 - sparse_categorical_accuracy: 0.9409 - val_loss: 0.2751 - val_sparse_categorical_accuracy: 0.9150\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.1417 - sparse_categorical_accuracy: 0.9441\n",
      "Epoch 00007: val_sparse_categorical_accuracy did not improve from 0.92125\n",
      "100/100 [==============================] - 117s 1s/step - loss: 0.1417 - sparse_categorical_accuracy: 0.9441 - val_loss: 0.2535 - val_sparse_categorical_accuracy: 0.9112\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.1264 - sparse_categorical_accuracy: 0.9563\n",
      "Epoch 00008: val_sparse_categorical_accuracy improved from 0.92125 to 0.93500, saving model to checkpoint2\\best_model_blond.h5\n",
      "100/100 [==============================] - 114s 1s/step - loss: 0.1264 - sparse_categorical_accuracy: 0.9563 - val_loss: 0.2723 - val_sparse_categorical_accuracy: 0.9350\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.1065 - sparse_categorical_accuracy: 0.9578\n",
      "Epoch 00009: val_sparse_categorical_accuracy did not improve from 0.93500\n",
      "100/100 [==============================] - 120s 1s/step - loss: 0.1065 - sparse_categorical_accuracy: 0.9578 - val_loss: 0.2748 - val_sparse_categorical_accuracy: 0.9212\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0939 - sparse_categorical_accuracy: 0.9691\n",
      "Epoch 00010: val_sparse_categorical_accuracy did not improve from 0.93500\n",
      "100/100 [==============================] - 113s 1s/step - loss: 0.0939 - sparse_categorical_accuracy: 0.9691 - val_loss: 0.3300 - val_sparse_categorical_accuracy: 0.9212\n"
     ]
    }
   ],
   "source": [
    "print(\"Fit model on training data\")\n",
    "history = model_blond.fit(\n",
    "    X_train,\n",
    "    y_blond_train,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    validation_split=0.2,\n",
    "    callbacks=call_back,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transsexual-material",
   "metadata": {},
   "source": [
    "### Classification for attribute \"Smiling\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "gorgeous-dodge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Smiling\n",
       "0    2594\n",
       "1    2406\n",
       "dtype: int64"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_smile=celebs[\"Smiling\"]\n",
    "celebs.groupby('Smiling').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charming-closer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train - test split\n",
    "\n",
    "X_train, X_test, y_smile_train, y_smile_test = train_test_split(my_array, y_smile, test_size=0.20, \n",
    "                                                                random_state=82, stratify=y_smile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-illness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# built the model\n",
    "model_blond = keras.Sequential()\n",
    "model_blond.add(layers.Conv2D(32, 3, activation=\"relu\", input_shape=(218, 178, 3), name=\"Conv1\"))\n",
    "model_blond.add(layers.MaxPooling2D(2,2))\n",
    "model_blond.add(layers.Conv2D(64, 3, activation=\"relu\", name=\"Conv2\"))\n",
    "model_blond.add(layers.MaxPooling2D(2,2))\n",
    "model_blond.add(layers.Conv2D(128, 3, activation=\"relu\", name=\"Conv3\"))\n",
    "model_blond.add(layers.MaxPooling2D(2,2))\n",
    "model_blond.add(layers.Conv2D(256, 3, activation=\"relu\", name=\"Conv4\"))\n",
    "model_blond.add(layers.MaxPooling2D(2,2))\n",
    "model_blond.add(layers.Flatten())\n",
    "model_blond.add(layers.Dense(256, activation=\"relu\", name=\"Dense1\"))\n",
    "model_blond.add(layers.Dense(64, activation=\"relu\", name=\"Dense2\"))\n",
    "model_blond.add(layers.Dense(2, activation=\"softmax\", name=\"Prediction\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-fault",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-spectrum",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biological-evidence",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-contrary",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
